{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP_RNN_Modele_de_Langage.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Réseaux Recursifs (RNNs) et Modèles de Langage\n",
        "Pour ce TP nous allons explorer les RNNs et notamment les LSTMs. Nous allons essayer d'utiliser un RNN pour apprendre des séquences des mots (un texte) et ensuite générer de nouvelles séquences. \n",
        "\n",
        "Néanmoins, avant cela nous allons examiner la structure basique d'un LSTM en utilisant une entrée aléatoire. Même si plus tard nous allons avoir du texte en entrée, les LSTMs fonctionnent avec des nombres. Nous allons voir plus tard comment passer du texte aux tenseurs. Pour l'instant voici un tenseur 3x8 aléatoire. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V5hmV8eaS9Nm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from collections import Counter\n"
      ],
      "metadata": {
        "id": "m5XQWEfa7w8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#entrée\n",
        "x = torch.randint(1, 100, (3, 8))\n",
        "print(x)\n"
      ],
      "metadata": {
        "id": "r7DqeDD-o96j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons passer cet entrée aléatoire à une couche embedding, parce que les embeddings des mots peuvent mieux  representer le contexte et sont plus efficaces que les representations one-hot. \n",
        "\n",
        "Pour Pytorch nous avons besoin d'utiliser `nn.Embedding` afin de créer cette couche, qui prend en entrée la taille du vocabulaire et la longueur de vecteur de mot souhaitée. Vous pouvez éventuellement fournir un index de padding, pour indiquer l'index de l'élément de padding dans la matrice qui va représenter une phrase. Le padding sert à mettre ensemble plusieurs phrases dans un minibatch pour les mettre toutes à la même longueur.\n",
        "\n",
        "\n",
        "Dans l'exemple suivant, notre vocabulaire se compose de 100 mots, incluant l'élément spécial du padding, pour lequel on a choisi de donner l'indice 0.\n",
        "\n",
        "Remarque : dans cet exemple, le padding ne sert pas...\n",
        "\n",
        "Que représente `x` dans notre contexte NLP ? Quelle est la taille des tenseurs issus de `model1` de la cellule suivante ?"
      ],
      "metadata": {
        "id": "b4gWwcV2qQFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = nn.Embedding(100, 7, padding_idx=0)\n",
        "out1 = model1(x)"
      ],
      "metadata": {
        "id": "uXLUDsdtrjH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous passons la sortie de la couche embedding dans une couche LSTM qui prend en entrée la longeur du vecteur representant le mot, la longueur de la couche cachée, et le nombre des couches. La couche LSTM sort trois choses, à quoi correspondent chacune d'entre elles ? Quelles sont leurs tailles ?"
      ],
      "metadata": {
        "id": "bWczP6zGr7-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = nn.LSTM(input_size=7, hidden_size=5, num_layers=1, batch_first=True)\n",
        "\n",
        "out, (ht, ct) = model2(out1)\n"
      ],
      "metadata": {
        "id": "SErXwFhftWgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passons maintaiannt au modèle de langage. Pour les données nous allons utiliser un ensemble de blagues recueillis sur Reddit, inspiré par un tutoriel de Domas Bitvinskas.   \n",
        "\n",
        "\n",
        "## Le modèle \n",
        "Voici un premier modèle utilisant trois couches LSTM."
      ],
      "metadata": {
        "id": "eA9OqdSCo-8-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfA98RjmS5ZJ"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        super(Model, self).__init__()\n",
        "        self.lstm_size = 128\n",
        "        self.embedding_dim = 128\n",
        "        self.num_layers = 3\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "C'est un model LSTM avec Pytorch assez standard. Comme expliqué dans l'introduction, le but de la couche `Embedding` est de convertir les mots (leur indice dans un dictionnaire) en vecteurs. La fonction `init_state` est appelée au début de chaque époque.  \n",
        "\n",
        "## Données \n",
        "Téléchargons les données.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YFW6l7vVS7Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/reddit-cleanjokes.csv"
      ],
      "metadata": {
        "id": "HkyqHu-J-xG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Il s'agit d'un fichier tsv de la forme `ID,Joke` ou `ID` signifie simplement l'identifiant de la « blague » et `Joke` le texte. Afin de pouvoir traiter les données nous aurons besoin d'utiliser la class `Dataset`\n"
      ],
      "metadata": {
        "id": "D_4Zpmks-tn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  \n",
        "    def __init__(\n",
        "        self,\n",
        "        sequence_length,\n",
        "    ):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.words = self.load_words()\n",
        "        self.uniq_words = self.get_uniq_words()\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "    \n",
        "    def load_words(self):\n",
        "        train_df = pd.read_csv('reddit-cleanjokes.csv')\n",
        "        text = train_df['Joke'].str.cat(sep=' ')\n",
        "        return text.split(' ')\n",
        "    \n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.sequence_length\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.sequence_length]),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1]),\n",
        "        )"
      ],
      "metadata": {
        "id": "Zy0v3k37Xht1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette classe hérite de la classe `torch.utils.data.Dataset`. En plus de `__init__`,  il est nécessaire de définir deux fonctions : `__len__` et `__getitem__`. La première retourne la taille de notre ensemble des données alors que la deuxième implémente l'indexation afin que `dataset[i]` puisse être utilisé pour retourner le *i*-ème élément. Vous pouvez avoir plus de détails [ici](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class). \n",
        "\n",
        "La fonction `load_words` charge le dataset. Le but est de trouver tous les mots afin de définir la taille du vocabulaire du réseau mais également la taille de l'embedding. Deux autres fonctions `index_to_word` et `word_to_index` convertissent les mots en index et vice versa. \n",
        "\n",
        "# Entrainement \n",
        "\n",
        "Nous allons définir une fonction `train` pour entraîner notre RNN. "
      ],
      "metadata": {
        "id": "J3gjVI4UhYxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "\n",
        "def train(dataset, model, max_epochs, sequence_length):\n",
        "    model.train()\n",
        "    dataloader = DataLoader(dataset, batch_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    for epoch in range(max_epochs):\n",
        "        state_h, state_c = model.init_state(sequence_length)\n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })"
      ],
      "metadata": {
        "id": "tNCaFS1YziKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comme vous pouvez constater la fonction train charge d'abord les données, définit comme fonction de perte Cross Entropy Loss ainsi que SGD comme optimizer et ensuite appelle le modèle pour `max_epochs`. \n",
        "\n",
        "# Prédictions \n",
        "\n",
        "Une fois que nous avons entraîné notre modèle nous pouvons ensuite faire des prédictions, en donnant une séquence des mots en entrée.   Voici une fonction nous permettant de prédire les `next_words` suivant à partir d'un `text`. "
      ],
      "metadata": {
        "id": "mF6YTawU0pg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(dataset, model, text, next_words=100):\n",
        "    model.eval()\n",
        "    words = text.split(' ')\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "    return words"
      ],
      "metadata": {
        "id": "CuvVMi4F0ptq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous sommes prêtes et prêts maintenant à entraîner notre modèle, ci-dessous un morceau du code qui nous permettra de le faire sur l'ensemble des données."
      ],
      "metadata": {
        "id": "JzDYV1ff0vNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 4\n",
        "max_epochs = 10\n",
        "\n",
        "dataset = Dataset(sequence_length)\n",
        "model = Model(dataset)\n",
        "print(model)\n",
        "train(dataset, model, max_epochs, sequence_length)\n",
        "print(predict(dataset, model, text='One day I shot an elephant in my suit. I have no idea how he got into it.'))"
      ],
      "metadata": {
        "id": "HnhUFjd70vXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Exercises \n",
        "\n",
        "1.  Nous allons essayer d'améliorer les résultats de notre réseau récurrent. Essayez de changer la taille de la représentation cachée (hidden representation size). Les résultats sont-ils meilleurs ?"
      ],
      "metadata": {
        "id": "CTad20L1xHwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uzE7WGnHxkuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Essayez d'expérimenter avec une couche récurrente supplémentaire. Avez-vous obtenu de meilleurs résultats ? "
      ],
      "metadata": {
        "id": "38mhNK_mxlRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6GzFjoiGxqLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Expérimentez également avec le dropout rate et essayez de voir si vos résultats sont meilleurs. "
      ],
      "metadata": {
        "id": "_EKT_ojv7sIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1oPxi81W7u2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Transformez le LSTM en un LSTM bidirectionnel \n"
      ],
      "metadata": {
        "id": "CJNSMACkE6CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xgpSvjRJFPzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. La fonction `predict` choisit aléatoirement, selon la distribution des probabilités, un mot. Essayez de faire la même chose, mais en se limitant sur les 4 mots les plus probables. Autrement dit, utilisez une distribution de probabilités uniforme sur ces 4 mots. Les résultats se sont-ils améliorés ? Que doit-on faire pour les améliorer encore ?\n",
        "\n"
      ],
      "metadata": {
        "id": "0DOjLHSAPguS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bno8VCGcPwjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Vous pouvez télécharger un autre jeu de données, des recettes de cuisine. Tester le modèle sur ces nouvelles données."
      ],
      "metadata": {
        "id": "6GhVuQHtADBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.irit.fr/~Thomas.Pellegrini/ens/M1ML1/sents_recipes.txt"
      ],
      "metadata": {
        "id": "C-bt8alRAVpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Voici un troisième jeu de données : une liste de noms de villes françaises. Cette fois-ci, l'idée est de travailler au niveau des caractères plutôt que des mots. Modifier le notebook pour gérer les caractères. La taille de la séquence à modéliser peut être plus grande que pour les mots. \n"
      ],
      "metadata": {
        "id": "3uDj2YW-AYXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.irit.fr/~Thomas.Pellegrini/ens/M1ML1/communes_france.txt"
      ],
      "metadata": {
        "id": "fuq-FtVoA9T8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}